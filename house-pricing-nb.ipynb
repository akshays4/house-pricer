{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Project Stage 2","metadata":{}},{"cell_type":"markdown","source":"# Data Analysis in Predict House Pricing","metadata":{}},{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"The aim of this notebook is to build some models that can predict NSW housing prices based on a set of scrapped features made available in the NSW Housing Dataset.\nThe current dataset has recently been updated, so it's interesting to explore the significance of new features, and their impact on model accuracy.","metadata":{}},{"cell_type":"markdown","source":"We obtained two dataset from Kaggle (www.kaggle.com/datasets/karltse/sydney-suburbs-reviews) (www.kaggle.com/datasets/alexlau203/sydney-house-prices). As a continuation of the stage 1, both datasets have information that will help us to analyse and answer some questions. The purpose of the stage 2 is to answer the questions from the previous stage.","metadata":{}},{"cell_type":"markdown","source":"## Questions","metadata":{}},{"cell_type":"markdown","source":"How to compare the performance of different machine learning models?\nWhat is the predicted prices and actual prices of the houses and elaborate the relation and trends?\nWhat does Sydney houses pricing dataset show about the prices of the houses over different period?\nWhat is overall trend of highest and lowest percentage of ethnics of language speakers in five different suburbs?\nWhat is the relationship between the population of a suburb and its review score?\nWhat can be the main factor of properties expensiveness?\nWhat is the relation between sales prices of home having low to higher numbers of beds and prices of home which are near to CBD?\nWhat is the relationship between the sales price of properties in Sydney and factors such as the type of region, number of beds, property size, distance from the CBD, median income of the suburb, and population of the suburb?\nHow do the house prices in Sydney vary by suburb according to the data in the \"sydney-house-prices\" dataset?","metadata":{}},{"cell_type":"markdown","source":"## Description of the Data","metadata":{}},{"cell_type":"markdown","source":"#### Description of Dataset 1 \nDataset1=pd.read_csv('domain_properties.csv')\n\n1. price is the price of the property in AUD\n2. date_sold is the date the property was sold\n3. suburb is the suburb the property is situated in\n4. num_bath is the number of bathrooms in the property\n5. num_bed is the number of bedrooms in the property\n6. num_parking is the number of parking spaces on the property\n7. property_size is the size of the property in square metres\n8. type is the type of building\n9. suburb_population is the population of the suburb the property is situated in\n10. suburb_median_income is the median income of the suburb the property is situated in\n11. srburb_lat is the latitude of the suburb that the property is situated in\n12. suburb_lng is the longitude of the suburb that the property is situated in\n13. suburb_elevation is the elevation of the suburb that the property is situated in.\n14. cash_rate is the cash rate at the time the property was sold\n15. property_inflation_index is the residential property price inflation index of the quarter that the property was sold in\n16. km_from_cbd is the distance between the property and the centre of Sydney CBD.\n\n#### Description of Dataset 2\nDataset2=pd.read_csv('Sydney Suburbs Reviews.csv')\n\n1. Name is suburb name\n2. Region is NSW regions\n3. Population (rounded)* is suburb population\t\n4. Postcode is Postcode in each suburbs\n9. Ethnic Breakdown 2016 is type of ethnic in NSW                         \n10. Median House Price (2020) is median properties price in 2020                 \n11. Median House Price (2021)  is median properties price in 20201                    \n12. % Change is price between 2020-2021                                     \n13. Median House Rent (per week) is renting house fee per week                   \n14. Median Apartment Price (2020) is selling apartment fee in 2020            \n15. Median Apartment Rent (per week)is renting apartment fee per week                 \n16. Public Housing % is % of public housing                               \n17. Avg. Years Held is heldig avg years                               \n18. Time to CBD (Public Transport) [Town Hall St] is duration of transport to CBD\n19. Time to CBD (Driving) [Town Hall St] is duration of driving to CBD         \n20. Nearest Train Station is nearest train station                        \n21. Highlights/Attractions is popular palace                       \n22. Ideal for is type of ideal                                      \n23. Traffic is avg of traffic jam                                         \n24. Public Transport is rating of Public Transport                            \n25. Affordability (Rental)is rating of Affordability (Rental)                        \n26. Affordability (Buying)is rating of Affordability (Buying)                         \n27. Nature is rating of nature                                        \n28. Noise is rating of noise                                           \n29. Things to See/Do is rating of Things to see                                \n30. Family-Friendliness is rating of Family-Friendliness                             \n31. Pet Friendliness is rating of Pet Friendliness                               \n32. Safety is rating of Safety                                        \n33. Overall Rating is rating of Overall                                  \n34. Review Link is rating of website link                      ","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"markdown","source":"Loading Data","metadata":{}},{"cell_type":"markdown","source":"In the below. two data set has been uploaded. Sydney suburb reviews and second one is domain properties. while in second data sheet it has two years data 2020 and 2021 including suburb details about ethnics with different languages and the average change of prices between 2020 and 2021.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport re\nimport plotly.express as px\nimport seaborn as sns\n\nDataset1=pd.read_csv('domain_properties.csv',header=0)\nDataset2=pd.read_csv('Sydney Suburbs Reviews.csv',header=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first data set shows sydeny different suburb housing costs, property sizes, when they are sold, number of baths, bedrooms, parking slots, suburb population and also suburb average income with property inflation index.","metadata":{}},{"cell_type":"code","source":"data_price = Dataset1.copy()\ndata_price","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The second data set shows the data of different suburb with general regions indicating population with ethnic breakdown with two years (2020 and 2021) different prices. Furthermore the average distance to nearest station, most attractive places around. trafffic and public transport points etc...","metadata":{}},{"cell_type":"code","source":"data_ethnic=Dataset2.copy()\ndata_ethnic.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore Feature","metadata":{}},{"cell_type":"code","source":"#dataset 1\ndata_price.info()\nprint('Number of instances = %d' % (data_price.shape[0]))\nprint('Number of attributes = %d' % (data_price.shape[1]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset 2\ndata_ethnic.info()\nprint('Number of instances = %d' % (data_ethnic.shape[0]))\nprint('Number of attributes = %d' % (data_ethnic.shape[1]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset 1\ndata_price.describe().T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset 2\ndata_ethnic.describe().T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing data","metadata":{}},{"cell_type":"code","source":"#dataset 1\nprint('Counting missing data for each feature')\ndata_price.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"in this they are counting missing data for each of the feature. so as we can see there are different missing data. there are no mmissing data and also there are many of missing data also. ","metadata":{}},{"cell_type":"code","source":"#dataset 2\nprint('Counting missing data for each feature')\ndata_ethnic.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualizing missing data","metadata":{}},{"cell_type":"markdown","source":"Here Dataset2 is showing the visual image of the missing data. in form of number vertically and different factors horizontally ","metadata":{}},{"cell_type":"code","source":"print('Visualizing missing data')\nmsno.bar(data_ethnic)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning ","metadata":{}},{"cell_type":"markdown","source":"#### Cleaning Dataset2","metadata":{}},{"cell_type":"markdown","source":"in this the overall data cleaning has been started with removing properties other than houses, removing suburbs having postal code except those start with 2 having four numbers. replacing missing values by zero. handing outliers which are lied outside the zone. and also at the end creating a new coloumn for analysing. ","metadata":{}},{"cell_type":"markdown","source":"Data set 2 has been claned the same way as above. just leaving rounded population of suburbs. average years held, median prices, things to see, family frindlinesss, safety etc... ","metadata":{}},{"cell_type":"markdown","source":"Cleaning ethnic data with different languages which includes chinese speaking english australian irish and many more. ","metadata":{}},{"cell_type":"code","source":"clean_data_ethnic=data_ethnic.drop(['Population (rounded)*','Avg. Years Held','Median House Price (2020)','Median House Price (2021)','% Change','Median House Rent (per week)','Median Apartment Price (2020)','Median Apartment Rent (per week)','Highlights/Attractions','Time to CBD (Public Transport) [Town Hall St]','Time to CBD (Driving) [Town Hall St]','Public Housing %','Median House Price (2020)','Median House Price (2021)','% Change','Median Apartment Price (2020)','Ideal for','Traffic','Public Transport','Affordability (Rental)','Affordability (Buying)',\n            'Nature','Noise','Things to See/Do','Family-Friendliness','Pet Friendliness','Safety','Review Link', 'Overall Rating','Nearest Train Station'], axis=1)\nclean_data_ethnic","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(clean_data_ethnic)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"#### Rename column for consistency","metadata":{}},{"cell_type":"code","source":"clean_data_ethnic.rename(columns={'Name':'Suburb', 'Ethnic Breakdown 2016':'Ethnic'}, inplace=True)\ncethnic=clean_data_ethnic.copy()\ncethnic","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Splitting column 'Ethnic'","metadata":{}},{"cell_type":"code","source":"ethic_english=[]\nfor ethnic in cethnic['Ethnic']:\n    num = re.search(r'English'+'\\s+(\\d*\\.\\d+|\\d+)', ethnic)\n    ethic_english.append(str(num))\n\nethic_australian=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Australian\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_australian.append(str(num))\n\nethic_chinese=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Chinese\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_chinese.append(str(num)) \nethic_greek=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Greek\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_greek.append(str(num))\nethic_irish=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Irish\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_irish.append(str(num))\nethic_indian=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Indian\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_indian.append(str(num))\nethic_scottish=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Scottish\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_scottish.append(str(num))\nethic_italian=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Italian\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_italian.append(str(num))\nethic_nepalese=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Nepalese\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_nepalese.append(str(num))\nethic_korean=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Korean\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_korean.append(str(num))\nethic_lebanese=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Lebanese\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_lebanese.append(str(num))\nethic_mongolian=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Mongolian\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_mongolian.append(str(num))\nethic_chineseScottish=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'ChineseScottish\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_chineseScottish.append(str(num))\nethic_vietnamese=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Vietnamese\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_vietnamese.append(str(num))\nethic_thai=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Thai\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_thai.append(str(num))\n    ethic_filipino=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Filipino\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_filipino.append(str(num))\n\n    ethic_turkish=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Turkish\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_turkish.append(str(num))\n    ethic_iraqi=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Iraqi\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_iraqi.append(str(num))\n    ethic_maltese=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Maltese\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_maltese.append(str(num))\n    ethic_khmerCambodian=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'KhmerCambodian\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_khmerCambodian.append(str(num))\n    ethic_assyrian=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Assyrian\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_assyrian.append(str(num))\n    ethic_bangladeshi=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Bangladeshi\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_bangladeshi.append(str(num))\n    ethic_indonesian=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Indonesian\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_indonesian.append(str(num))\n    ethic_sriLankan=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'SriLankan\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_sriLankan.append(str(num))\n    ethic_samoan=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Samoan\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_samoan.append(str(num))\n    ethic_german=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'German\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_german.append(str(num))\n    ethic_scottishChinese=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'ScottishChinese\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_scottishChinese.append(str(num))\n    ethic_macedonian=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Macedonian\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_macedonian.append(str(num))\n    ethic_afghann=[]\nfor ethinc in cethnic['Ethnic']:\n    num = re.search(r'Afghan\\s+(\\d*\\.\\d+|\\d+)', ethinc)\n    ethic_afghann.append(str(num))\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add 2 new columns to the dataframe\ncethnic['English'] = ethic_english\ncethnic['Australian'] = ethic_australian\ncethnic['Chinese'] = ethic_chinese\ncethnic['Greek'] = ethic_greek\ncethnic['Irish'] = ethic_irish\ncethnic['Indian'] = ethic_indian\n\ncethnic['Scottish']=ethic_scottish\n\ncethnic['Italian']=ethic_italian \n \ncethnic['Nepalese']=ethic_nepalese\n \ncethnic['Korean']=ethic_korean\n\ncethnic['Lebanese']=ethic_lebanese\n\ncethnic['Mongolian']=ethic_mongolian\n\ncethnic['ChineseScottish']=ethic_chineseScottish\n\ncethnic['Vietnamese']=ethic_vietnamese\n\ncethnic['Thai']=ethic_thai\n\ncethnic['Filipino']=   ethic_filipino\n\n\ncethnic['Turkish']=    ethic_turkish\n\ncethnic['Iraqi']=    ethic_iraqi\n\ncethnic['Maltese']=    ethic_maltese\n\ncethnic['KhmerCambodian']=    ethic_khmerCambodian\n\ncethnic['Assyrian']=    ethic_assyrian\n\ncethnic['Bangladeshi']=    ethic_bangladeshi\n\ncethnic['Indonesian']=    ethic_indonesian\n\ncethnic['SriLankan']=    ethic_sriLankan\n\ncethnic['Samoan']=    ethic_samoan\n\ncethnic['German']=    ethic_german\n\ncethnic['ScottishChinese']=    ethic_scottishChinese\n\ncethnic['Macedonian']=    ethic_macedonian\n\ncethnic['Afghan']=    ethic_afghann\n\ncethnic\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cethnic['English'] = cethnic['English'].str[-6:-2]\ncethnic['Australian'] = cethnic['Australian'].str[-6:-2]\ncethnic['Chinese'] = cethnic['Chinese'].str[-6:-2]\ncethnic['Greek'] = cethnic['Greek'] .str[-6:-2]\ncethnic['Irish'] = cethnic['Irish'].str[-6:-2]\ncethnic['Indian'] = cethnic['Indian'].str[-6:-2]\ncethnic['Scottish']=cethnic['Scottish'].str[-6:-2]\n\ncethnic['Italian']=cethnic['Italian'].str[-6:-2]\n \ncethnic['Nepalese']=cethnic['Nepalese'].str[-6:-2]\n \ncethnic['Korean']=cethnic['Korean'].str[-6:-2]\n\ncethnic['Lebanese']=cethnic['Lebanese'].str[-6:-2]\n\ncethnic['Mongolian']=cethnic['Mongolian'].str[-6:-2]\n\ncethnic['ChineseScottish']=cethnic['ChineseScottish'].str[-6:-2]\n\ncethnic['Vietnamese']=cethnic['Vietnamese'].str[-6:-2]\n\ncethnic['Thai']=cethnic['Thai'].str[-6:-2]\n\ncethnic['Filipino']=cethnic['Filipino'].str[-6:-2]\n\n\ncethnic['Turkish']=cethnic['Turkish'].str[-6:-2]\n\ncethnic['Iraqi']=cethnic['Iraqi'].str[-6:-2]\n\ncethnic['Maltese']=cethnic['Maltese'].str[-6:-2]\n\ncethnic['KhmerCambodian']= cethnic['KhmerCambodian'].str[-6:-2]\n\ncethnic['Assyrian']=cethnic['Assyrian'].str[-6:-2]\n\ncethnic['Bangladeshi']=cethnic['Bangladeshi'].str[-6:-2]\n\ncethnic['Indonesian']=cethnic['Indonesian'].str[-6:-2]\n\ncethnic['SriLankan']=cethnic['SriLankan'].str[-6:-2]\n\ncethnic['Samoan']=cethnic['Samoan'].str[-6:-2]\n\ncethnic['German']=cethnic['German'].str[-6:-2]\n\ncethnic['ScottishChinese']=cethnic['ScottishChinese'].str[-6:-2]\ncethnic['Macedonian']=cethnic['Macedonian'].str[-6:-2]\n\ncethnic['Afghan']=cethnic['Afghan'].str[-6:-2]\ncethnic","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cethnic['Population'] = cethnic['Population'].apply(pd.to_numeric,errors='coerce')\ncethnic['English'] = cethnic['English'].apply(pd.to_numeric,errors='coerce')\ncethnic['Australian'] = cethnic['Australian'].apply(pd.to_numeric,errors='coerce')\ncethnic['Chinese'] = cethnic['Chinese'].apply(pd.to_numeric,errors='coerce')\ncethnic['Greek'] = cethnic['Greek'].apply(pd.to_numeric,errors='coerce')\ncethnic['Irish'] = cethnic['Irish'].apply(pd.to_numeric,errors='coerce')\ncethnic['Indian'] = cethnic['Indian'].apply(pd.to_numeric,errors='coerce')\ncethnic['Scottish']=cethnic['Scottish'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['Italian']=cethnic['Italian'].apply(pd.to_numeric,errors='coerce')\n \ncethnic['Nepalese']=cethnic['Nepalese'].apply(pd.to_numeric,errors='coerce')\n \ncethnic['Korean']=cethnic['Korean'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['Lebanese']=cethnic['Lebanese'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['Mongolian']=cethnic['Mongolian'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['ChineseScottish']=cethnic['ChineseScottish'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['Vietnamese']=cethnic['Vietnamese'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['Thai']=cethnic['Thai'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['Filipino']=cethnic['Filipino'].apply(pd.to_numeric,errors='coerce')\n\n\ncethnic['Turkish']=cethnic['Turkish'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['Iraqi']=cethnic['Iraqi'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['Maltese']=cethnic['Maltese'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['KhmerCambodian']= cethnic['KhmerCambodian'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['Assyrian']=cethnic['Assyrian'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['Bangladeshi']=cethnic['Bangladeshi'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['Indonesian']=cethnic['Indonesian'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['SriLankan']=cethnic['SriLankan'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['Samoan']=cethnic['Samoan'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['German']=cethnic['German'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['ScottishChinese']=cethnic['ScottishChinese'].apply(pd.to_numeric,errors='coerce')\ncethnic['Macedonian']=cethnic['Macedonian'].apply(pd.to_numeric,errors='coerce')\n\ncethnic['Afghan']=cethnic['Afghan'].apply(pd.to_numeric,errors='coerce')\n#cethnic=cethnic.convert_dtypes()\ncethnic.info()\ncethnic","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_ethnic_clean=cethnic.drop(['Ethnic'],axis='columns')\ndata_ethnic_clean.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"here we have to find any duplicate rows but as you see there are no duplications. ","metadata":{}},{"cell_type":"markdown","source":"## Duplicate Data","metadata":{}},{"cell_type":"code","source":"#cethnic[cethnic.duplicated([\"Suburb\",'Region','Postcode','Greek'], keep=False)]\ndup_ethnic = cethnic.duplicated()\nprint('Number of duplicate rows = %d' % (dup_ethnic.sum()))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we tried to find duplicate values in term of the price but there are no duplications. ","metadata":{}},{"cell_type":"code","source":"dups = data_price.duplicated()\nprint('Number of duplicate rows = %d' % (dups.sum()))\ndata_price.loc[[0,100]]","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data merging  ","metadata":{}},{"cell_type":"markdown","source":"Here both the two of the data set has been merged and we got a huge data combined. inlcuing 7000 rows and 48 coloums. from \nhere we got the data of different suburb prices, data sold, property sizes. population, suburb median income, and the suburb \nlies within which region and most important each and every ethic group people which are English speaking, Austrlian, Irish, \nIndian and many more. ","metadata":{}},{"cell_type":"code","source":"data_merge = data_price.merge(data_ethnic_clean[:], left_on='suburb',\n                  right_on='Suburb').drop('Suburb', axis='columns')\ndata_merge","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show all columns","metadata":{}},{"cell_type":"markdown","source":"We´ll drop this one, it carries no relevant information at all.","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_merge.drop(['Region','Postcode'],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Fill null to 0","metadata":{}},{"cell_type":"code","source":"#fill null to 0\ndata_merge = data_merge.fillna(0)\ndata_merge","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_price[(data_price['num_bath']==0) | (data_price['num_bed']==0)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Correct data types","metadata":{}},{"cell_type":"code","source":"# Correct data types\ndata_merge['date_sold'] = pd.to_datetime(data_merge['date_sold'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis","metadata":{}},{"cell_type":"code","source":"test= data_merge.drop_duplicates(subset=[\"suburb\"],keep='first')\ntest","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"taking data of those areas having highest level of English speakers. ","metadata":{}},{"cell_type":"code","source":"largest_english=test.nlargest(n=10\n                            , columns=['English'])\nlargest_english","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bar graph showing highest to lowest ethnics with English at top and with the top most highest suburb in term of ethnic presence. ","metadata":{}},{"cell_type":"markdown","source":"#### 7.What is overall trend of highest and lowest percentage of ethnics of language speakers in 10 different suburbs?","metadata":{}},{"cell_type":"code","source":"\nfig = px.bar(largest_english,x='suburb',  y=[\"English\", \"Australian\",'Chinese','Irish','Scottish','Italian','German'], text_auto='.2s',\n            title=\"Default: various text sizes, positions and angles\")\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As it can be seen clearly there is a very slight change in speaker of English in 10 different suburbs and also we can say there is no change in the lowest language speakers which are German. There is fluctuations in both of them but can be seem no huge changes are there in any of these.","metadata":{}},{"cell_type":"code","source":"#Numerical Dataset\ndf_numerical = data_price.drop(['suburb','type','property_inflation_index','cash_rate','suburb_elevation'],axis=1)\ndf_numerical","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What does Sydney houses pricing dataset show about the prices of the houses over different period?","metadata":{}},{"cell_type":"code","source":"plt.scatter(data_merge.date_sold, data_merge.price, marker='o', c='b',edgecolor='r', alpha=0.5)\nplt.title(\"Sydney housing price over years\")\nplt.grid()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this dataset there is shown from the year 2016 to 2022. The prices on 2016 is very low and the peak value is almost 0.25 and in 2022 the values are increasing and the highest values are 1.65 and 1.75 which is a huge change and with this graph it is clearly mentioned that the prices are going very high with each year passing.","metadata":{}},{"cell_type":"markdown","source":"#### How do the house prices in Sydney vary by suburb according to the data in the \"sydney-house-prices\" dataset","metadata":{}},{"cell_type":"code","source":"suburbs = data_merge.groupby(['suburb'])['price'].aggregate('median')\nprint(suburbs)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"suburbs = pd.DataFrame(suburbs).reset_index()\nsuburbs.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = []\n\nfor i in suburbs['price']:\n    if i <= suburbs['price'].quantile(0.25):\n        results.append('low')\n    elif i > suburbs['price'].quantile(0.25) and i <= suburbs['price'].quantile(0.50):\n        results.append('medium_low')\n    elif i > suburbs['price'].quantile(0.50) and i <= suburbs['price'].quantile(0.75):\n        results.append('medium_high')\n    else:\n        results.append('high')\n        \n    \n# add to new dataframe\nsuburbs['suburb_group'] = results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"suburbs.head()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the dataset, it appears that the house prices in Sydney vary greatly by suburb. Some suburbs have a high median price (e.g. Abbotsford), while others have a medium-high median price (e.g. Allambie Heights, Allawah). Additionally, some suburbs have a medium-low median price (e.g. Abbotsbury, Alexandria), while others have a low median price. This suggests that the house prices in Sydney can vary significantly depending on the suburb.","metadata":{}},{"cell_type":"code","source":"# converting into a dictionary\nsuburb = suburbs['suburb']\nsuburb_group = suburbs['suburb_group']\nsuburb_dict = dict(zip(suburb, suburb_group))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting to an array to fill with grouped values\nconvert = data_merge.suburb\nconvert = list(convert)\n\n\n# conversion into an array\nnew_list = []\n\nfor conversion in convert:\n    for check in suburb_dict:\n        if conversion == check:\n            new_list.append(suburb_dict[check])\nprint(f\"Length of dataframe : {len(data_merge)}\")\nprint(f\"Length of array : {len(new_list)}\")","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_merge['suburb_group'] = new_list\ndata_merge.head()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_merge['type'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_merge['type'].unique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\n# Create a Figure and an Axes with plt.subplots\nfig, ax = plt.subplots(1,1,figsize=(20,15),sharex=False)\n\n#county_order = data_merge.sort_values('price')['Region']\n\n#sns.barplot(x='State', y='Headcount Ratio (%)', data=gdf_regions, ax=ax1, color='c', )\n\nsns.barplot(ax=ax,data =data_merge, x=\"type\", y=\"price\" )\nax.set_ylabel(\"price\")\nax.set_xlabel(\" Type of properties \")\nax.title.set_text(\"Sales Price and  type\")\nplt.xticks(rotation='vertical')\nplt.show()\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting to array \ntypes = data_merge['type'].to_list()\n\n# groupings\nhouse = ['House', 'New House & Land', 'Villa','Duplex','Terrace']\napartments = ['Apartment / Unit / Flat', 'Townhouse', 'Studio']\n\n# new array\nnew_types = []\n\nfor conversion in types:\n    if conversion in house:\n        new_types.append('house')\n    elif conversion in apartments:\n        new_types.append('apartments')\n    else:\n        new_types.append('other')\n\n# inputting into dataframe\ndata_merge['type'] = new_types","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig, ax = plt.subplots(1,1,figsize=(16, 6), dpi=100)\nsns.set_theme(style='whitegrid')\nsns.despine()\nsns.kdeplot(data_merge[data_merge['type']=='house']['price'], color='#1EAE98', ax=ax, label='House')\nsns.kdeplot(data_merge[data_merge['type']=='apartments']['price'], color='#5800FF', ax=ax, label='Apartments')\nsns.kdeplot(data_merge[data_merge['type']=='other']['price'], color='#B8B5FF', ax=ax, label='Other')\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### What is the relationship between the sales price of properties in Sydney and factors such as the type of region, number of beds, property size, distance from the CBD, median income of the suburb, and population of the suburb?","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nsqft_trend = pd.concat([data_merge['price'], data_merge['property_size']], axis = 1)\nfig = px.scatter(data_merge, x = 'property_size', y = 'price', title = 'Price vs Property Size', labels = 'dict(price = \"Price \", sqft_living = \"Sqft \")')\nfig.update_layout(yaxis_range = [0 , 16500000], xaxis_range = [0 , 7000], width = 800, height = 600)\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nsqft_trend = pd.concat([data_merge['price'], data_merge['km_from_cbd']], axis = 1)\nfig = px.scatter(data_merge, x = 'km_from_cbd', y = 'price', title = 'Price vs KM from CBD', labels = dict(price = \"Price \", sqft_living = \"Sqft \"))\nfig.update_layout(yaxis_range = [0 , 16500000], xaxis_range = [0 , 60], width = 800, height = 600)\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sqft_trend = pd.concat([data_merge['price'], data_merge['date_sold']], axis = 1)\nfig = px.scatter(data_merge, x = 'date_sold', y = 'price', title = 'Price vs Date Sold', labels = dict(price = \"Price \", sqft_living = \"Sqft \"))\nfig.update_layout(yaxis_range = [0 , 16500000], xaxis_range = [\"2016\", \"2022\"], width = 800, height = 600)\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\n# Create a Figure and an Axes with plt.subplots\nfig, ax = plt.subplots(1,1,figsize=(20,15),sharex=False)\n\n#county_order = data_merge.sort_values('price')['Region']\n\n#sns.barplot(x='State', y='Headcount Ratio (%)', data=gdf_regions, ax=ax1, color='c', )\n\nsns.barplot(ax=ax,data =data_merge, x=\"Region\", y=\"price\" )\nax.set_ylabel(\"Region\")\nax.set_xlabel(\" Type of region\")\nax.title.set_text(\"Sales Price and  region\")\nplt.xticks(rotation='vertical')\nplt.show()\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\n# Create a Figure and an Axes with plt.subplots\nfig, ax = plt.subplots(3,2,figsize=(18,15))\n\nsns.barplot(ax=ax[0, 0],data =largest_bedroom, x=\"num_bed\", y=\"price\")\nax[0,0].set_ylabel(\"Sales Price\")\nax[0,0].set_xlabel(\" Type of number of bed\")\nax[0,0].title.set_text(\"Sales Price and  number of bed\")\n\nsns.barplot(ax=ax[0, 1],data =data_merge, x=\"num_bath\", y=\"price\")\nax[0,1].set_ylabel(\"Sales Price\")\nax[0,1].set_xlabel(\"Number of bath\")\nax[0,1].title.set_text(\"Sales Price and  number of bath\")\n\nsns.barplot(ax=ax[1, 0],data =data_merge, x=\"num_parking\", y=\"price\")\nax[1,0].set_ylabel(\"Sales Price\")\nax[1,0].set_xlabel(\"Number of parking\")\nax[1,0].title.set_text(\"Sales Price and  number of parking\")\n\n\nsns.barplot(ax=ax[1, 1],data =data_merge, x=\"type\", y=\"price\")\nax[1,0].set_ylabel(\"Sales Price\")\nax[1,0].set_xlabel(\"type\")\nax[1,0].title.set_text(\"Sales Price and type\")\n\n\nsns.barplot(ax=ax[2, 0],data =data_merge, x=\"suburb_median_income\", y=\"price\")\nax[1,0].set_ylabel(\"Sales Price\")\nax[1,0].set_xlabel(\"suburb_median_income\")\nax[1,0].title.set_text(\"Sales Price and suburb_median_income\")\n\nsns.barplot(ax=ax[2, 1],data =data_merge, x=\"suburb_population\", y=\"price\")\nax[1,0].set_ylabel(\"Sales Price\")\nax[1,0].set_xlabel(\"suburb_population\")\nax[1,0].title.set_text(\"Sales Price and suburb population\")\n\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe the relationship between the sales price of properties in Sydney and various factors such as the type of region, number of beds, property size, distance from the CBD, median income of the suburb, and population of the suburb. The graphs display the average sales price on the y-axis and the different factors on the x-axis. From the graphs, we can see if there is a positive or negative correlation between the sales price and each factor, such as if higher median income in the suburb leads to higher sales prices or if properties closer to the CBD have higher sales prices compared to those further away.","metadata":{}},{"cell_type":"markdown","source":"## Latitude and Longitud relationship with Map\n","metadata":{}},{"cell_type":"code","source":"from folium import Map\nfrom folium.plugins import HeatMap\nheat_data = [[row['suburb_lat'],row['suburb_lng']] for _, row in data_merge.iterrows()]\nheat_map = Map(data_merge[['suburb_lat', 'suburb_lng']].mean(axis=0), zoom_start=10) \nHeatMap(heat_data, radius=10).add_to(heat_map)\nheat_map","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is the relationship between the population of a suburb and its review score?\n### What can be the main factor of properties expensiveness?","metadata":{}},{"cell_type":"code","source":"import branca\nimport folium\ninferno_colors = [\n    (0, 0, 4),\n    (40, 11, 84),\n    (101, 21, 110),\n    (159, 42, 99),\n    (212, 72, 66),\n    (245, 125, 21),\n    (250, 193, 39),\n    (252, 255, 164)\n]\n\nmap = folium.Map(data_merge[['suburb_lat', 'suburb_lng']].mean(axis=0), zoom_start = 11)\nlat = list(data_merge.suburb_lat)\nlon = list(data_merge.suburb_lng)\npopulations = list(data_merge.suburb_population)\ntargets = list(data_merge.price)\n \n# define colormap using inferno colors and normalizing them according MedHouseVal\ncmap = branca.colormap.LinearColormap(\n    inferno_colors, vmin=min(targets), vmax=max(targets)\n)\n\nfor loc, population, target in zip(zip(lat, lon), populations, targets):\n    folium.Circle(\n        location=loc,\n        radius=population/20,\n        fill=True,\n            color=cmap(target),\n        fill_opacity=0.5,\n        weight=0\n    ).add_to(map)\n\nmap.add_child(cmap)\ndisplay(map)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is possible to observe the relationship between the population of a suburb and its review score by the size and colour of the circles. If larger circles have a darker colour, it will indicate that larger population suburbs have higher review scores, and vice versa. This visualization can help in understanding the relationship between the population of a suburb and its review score, allowing for insights into consumer opinions in the area. The graph shows that suburbs located further away from the CBD have a higher score.\n\nThose properties whether they are apartments, houses or any other type of properties which are near to CBD are much more expensive than those who has some distance from main CBD. And those properties which are nearer to beaches whether the beaches are near to cities or far from cities tend to be much more expensive as seen in the graph. \nBut those properties which are in CBD and near to sea are much more expensive than all others as they pose both the properties to be in CBD and at the side of the sea.\n","metadata":{}},{"cell_type":"markdown","source":"## Prepraring data for prediction","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"prediction_df= data_merge.copy\nprediction_df=data_merge.drop(['date_sold','suburb_lat','suburb_lng','suburb_group','Postcode','Region','type','suburb'], axis='columns')\nprediction_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=prediction_df.drop(['price'],axis=1)\ny=prediction_df['price']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=101)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training dataset and test dataset must be similar, usually have the same predictors or variables. They differ on the observations and specific values in the variables. If you fit the model on the training dataset, then you implicitly minimize error or find correct responses. The fitted model provides a good prediction on the training dataset. Then you test the model on the test dataset. If the model predicts good also on the test dataset, you have more confidence. You have more confidence since the test dataset is similar to the training dataset, but not the same nor seen by the model. It means the model transfers prediction or learning in real sense.\n\nSo,by splitting dataset into training and testing subset, we can efficiently measure our trained model since it never sees testing data before.Thus it's possible to prevent overfitting.\n\nI am just splitting dataset into 30% of test data and remaining 80% will used for training the model.","metadata":{}},{"cell_type":"code","source":"scaler=StandardScaler()\nScaled_X_train=scaler.fit_transform(X_train)\nScaled_X_test=scaler.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ALL_Types_of_Regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix\nimport numpy as np\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_model(model,X_train,X_test,y_train,y_test):\n    Scaled_X_train=scaler.fit_transform(X_train).round()\n    Scaled_X_test=scaler.transform(X_test).round()\n    model.fit(Scaled_X_train,y_train)\n    preds=model.predict(Scaled_X_test)\n    rmse=np.sqrt(mean_squared_error(y_test,preds))\n    mae=mean_absolute_error(y_test,preds)\n    print(f'MAE: {mae}')\n    print(f'RMSE: {rmse}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Linear_Regression\nlr_model=LinearRegression()\nrun_model(lr_model,X_train,X_test,y_train,y_test)\npreds=lr_model.predict(Scaled_X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"residuals=y_test-preds\nresiduals\nsns.scatterplot(x=y_test,y=residuals)\nsns.displot(residuals,kde=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy as sp\ng,ax=plt.subplots(figsize=(6,8),dpi=50)\n_=sp.stats.probplot(residuals,plot=ax)\nfinal_model=LinearRegression()\nfinal_model.fit(X_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape,y_train.shape,X_test.shape,y_test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is the predicted prices and actual prices of the houses and elaborate the relation and trends?","metadata":{}},{"cell_type":"code","source":"c = [i for i in range(1,2135,1)] # generating index \nfig = plt.figure(figsize=(6,4))\nplt.plot(c,y_test, color=\"blue\", linewidth=2.5, linestyle=\"-\") #Plotting Actual\nplt.plot(c,preds, color=\"red\",  linewidth=2.5, linestyle=\"-\") #Plotting predicted\nfig.suptitle('Actual and Predicted', fontsize=15)              # Plot heading \nplt.xlabel('Index', fontsize=18)                               # X-label\nplt.ylabel('Housing Price', fontsize=16)    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a huge difference between the actual and predicted prices of the houses. The Red indicates the predicted prices of the houses while the blue indicates the real prices. Although we can see there are some peaks also which indicates a very high real price. While on predicated there is a slight peak which shows a value of 0.6 only but on another hand the actual prices have more than 1.4 also.","metadata":{}},{"cell_type":"code","source":"print(final_model.coef_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(final_model.intercept_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### How to compare the performance of different machine learning models?","metadata":{}},{"cell_type":"code","source":"from time import time\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.ensemble import RandomForestRegressor\n\nrand_regr = RandomForestRegressor(n_estimators=400,random_state=0)\nstart = time()\nrand_regr.fit(X_train, y_train)\nend=time()\ntrain_time_rand=end-start\nrandom=rand_regr.score(X_test,y_test)\npredictions = rand_regr.predict(X_test)\nexp_rand = explained_variance_score(predictions,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nstart = time()\nest=GradientBoostingRegressor(n_estimators=400, max_depth=5,min_samples_split=2,learning_rate=0.1).fit(X_train, y_train)\nend=time()\ntrain_time_g=end-start\ngradient=est.score(X_test,y_test)\n\npred = est.predict(X_test)\nexp_est = explained_variance_score(pred,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\nstart = time()\nada=AdaBoostRegressor(n_estimators=50, learning_rate=0.2,loss='exponential').fit(X_train, y_train)\nend=time()\ntrain_time_ada=end-start\npred=ada.predict(X_test)\nadab=ada.score(X_test,y_test)\npredict = ada.predict(X_test)\nexp_ada = explained_variance_score(predict,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree  import DecisionTreeRegressor\ndecision=DecisionTreeRegressor()\nstart = time()\ndecision.fit(X_train, y_train)\nend=time()\ntrain_time_dec=end-start\ndecc=decision.score(X_test,y_test)\ndecpredict = decision.predict(X_test)\nexp_dec = explained_variance_score(decpredict,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparing Models on the basis of Model's Accuracy Score and Explained Variance Score of different models\nmodels_cross = pd.DataFrame({\n    'Model': ['Gradient Boosting','AdaBoost','Random Forest','Decision Tree'],\n    'Score': [gradient,adab,random,decc],\n     'Variance Score': [exp_est,exp_ada,exp_rand,exp_dec]})\n    \nmodels_cross.sort_values(by='Score', ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ANALYZING TRAINING TIME EACH MODEL ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nmodel = ['Adab54soost', 'GBOOST', 'Random forest', 'Decision Tree']\nTrain_Time = [\n    train_time_ada,\n    train_time_g,\n    train_time_rand,\n    train_time_dec\n    \n]\nindex = np.arange(len(model))\nplt.bar(index, Train_Time)\nplt.xlabel('Machine Learning Models', fontsize=15)\nplt.ylabel('Training Time', fontsize=15)\nplt.xticks(index, model, fontsize=10, )\nplt.title('Comparison of Training Time of all ML models')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above figure it is inferred that decision tree has taken negligible amount of time to train where as Randome forest has taken maximum time and it is yet obvious because as we increase the number of tree in this case training time will increase so we should look out for optimal model which has greater accuracy and less training time in comparison to other So, in this case GBoost is the best choice as its accuracy is highest and it is taking less time to train with accuracy.","metadata":{}},{"cell_type":"markdown","source":"##  Conculsion ","metadata":{}},{"cell_type":"markdown","source":"The two datasets provide valuable information about the real estate market in Sydney, Australia, and the suburbs in the area. The \"Sydney House Prices\" dataset includes information about the sale prices of properties in various suburbs, while the \"Sydney Suburbs Reviews\" dataset offers insight into the reputation and desirability of each suburb based on customer reviews. By analysing these datasets together, it is possible to draw some conclusions about the relationship between the cost of properties and the popularity or quality of a suburb as a place to live.\n\nFor example, the data might show that suburbs with higher-priced properties tend to receive higher ratings and positive reviews from customers. This could indicate that these areas are considered more desirable and better places to live. Conversely, suburbs with lower-priced properties may receive lower ratings or mixed reviews, which could indicate that they are not as highly regarded by customers.\n\nThis information can be useful for individuals who are looking to purchase a property in Sydney or considering moving to a new suburb. By examining the data on both the cost of properties and the reputation of the suburbs, potential buyers and residents can make more informed decisions based on their budget and lifestyle preferences. For example, those who prioritize affordability may be more likely to consider suburbs with lower-priced properties, while those who place a higher value on quality of life may be more interested in suburbs with higher ratings and positive reviews.\n\nIn conclusion, the two datasets offer valuable information about the Sydney real estate market and suburbs, and by analysing them together, it is possible to gain a better understanding of the relationship between property prices and suburb quality and make more informed decisions about where to live in Sydney.\n","metadata":{}},{"cell_type":"markdown","source":"## References ","metadata":{}},{"cell_type":"markdown","source":"\nAlexlau203. (n.d.). Sydney House Prices [Data set]. Kaggle. https://www.kaggle.com/datasets/alexlau203/sydney-house-prices.\n\n\nKarltse. (n.d.). Sydney Suburbs Reviews [Data set]. Kaggle. https://www.kaggle.com/datasets/karltse/sydney-suburbs-reviews.\n\n\nMcKinney, W. (2012). Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. O'Reilly Media, Inc. https://proquest-safaribooksonline-com.ezproxy.lib.ryerson.ca/book/programming/python/9781449319793.\n\n\nVanderPlas, J. (2016). Python Data Science Handbook: Essential Tools for Working with Data. O'Reilly Media, Inc. https://proquest-safaribooksonline-com.ezproxy.lib.ryerson.ca/book/programming/python/9781491912126.\n\n\nMatplotlib Development Team. (2021). Matplotlib. https://matplotlib.org.\n\n\nSeaborn Development Team. (2021). Seaborn: Statistical data visualization. https://seaborn.pydata.org.\n\n\nScipy Development Team. (2021). SciPy: Open source scientific tools for Python. https://scipy.org.\n","metadata":{}}]}